{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Softmax Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score #takes 2lists and compares them\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    \"\"\"\n",
    "    Compute the cost and derivative.\n",
    "    n_classes: the number of classes\n",
    "    input_size: the size N of the input vector\n",
    "    lambda_: weight decay parameter\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    labels: an M x 1 matrix containing the labels corresponding for the input data\n",
    "    \"\"\"\n",
    "    # k stands for the number of classes\n",
    "    # n is the number of features and m is the number of samples\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Matrix of indicator fuction with shape (k, m)\n",
    "    labels = np.reshape(labels,(m,))\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    # Cost function\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    # Gradient matrix with shape (k, n)\n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    # Unroll the gradient matrices into a vector\n",
    "    grad = grad.ravel()\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mygrad(J, theta,learning_rate, maxiter):\n",
    "    for i in range(maxiter):\n",
    "        _,grad = J(theta)\n",
    "        theta = theta - (learning_rate * grad)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_train(input_size, n_classes, lambda_,learning_rate, input_data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    \"\"\"\n",
    "    Train a softmax model with the given parameters on the given data.\n",
    "    Returns optimal theta, a vector containing the trained parameters\n",
    "    for the model.\n",
    "    input_size: the size of an input vector x^(i)\n",
    "    n_classes: the number of classes\n",
    "    lambda_: weight decay parameter\n",
    "    input_data: an N by M matrix containing the input data, such that\n",
    "                input_data[:, c] is the c-th input\n",
    "    labels: M by 1 matrix containing the class labels for the\n",
    "            corresponding inputs. labels[c] is the class label for\n",
    "            the c-th input\n",
    "    options (optional): options\n",
    "      options['maxiter']: number of iterations to train for\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    # Find out the optimal theta\n",
    "    #results = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "    #opt_theta = results['x']\n",
    "    opt_theta = mygrad(J, theta, learning_rate,maxiter=options[\"maxiter\"])\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_predict(model, data):\n",
    "    \"\"\"\n",
    "    model: model trained using softmax_train\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    pred: the prediction array.\n",
    "    \"\"\"\n",
    "    theta = model['opt_theta'] # Optimal theta\n",
    "    k = model['n_classes']  # Number of classes\n",
    "    n = model['input_size'] # Input size (number of features)\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Prediction values\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = loadmat('./Data/mnistTrainImages.mat')\n",
    "X = X['trainData'].T\n",
    "y = loadmat('./Data/mnistTrainLabels.mat')\n",
    "y = y['trainLabels']\n",
    "X_test = loadmat('./Data/mnistTestImages.mat')\n",
    "X_test = X_test['testData'].T\n",
    "y_test = loadmat('./Data/mnistTestLabels.mat')\n",
    "y_test = y_test['testLabels']\n",
    "print X.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28 # Size of input vector (MNIST images are 28x28)\n",
    "n_classes = 10       # Number of classes (MNIST images fall into 10 classes)\n",
    "lambda_ = 1e-4 # Weight decay parameter.... Hyperparameters - tweaks to increase convergence\n",
    "learning_rate  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'maxiter': 100, 'disp': True}\n",
    "model = softmax_train(input_size, n_classes, lambda_,learning_rate, X, y, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9083\n"
     ]
    }
   ],
   "source": [
    "pred = softmax_predict(model, X_test)\n",
    "y_test = y_test.tolist()\n",
    "pred = pred.tolist()\n",
    "print accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Observations:-***\n",
    "|S.No|No.of Iterations|Learning_Rate|Accuracy|\n",
    "|:--:|:--------------:|:-----------:|:-------:|\n",
    "| | | | |\n",
    "|1|100|1|90.87|\n",
    "|2|500|1|91.95|\n",
    "|3|1000|1|92.25|\n",
    "|4|100|1e-1|87.09|\n",
    "|5|500|1e-1|89.94|\n",
    "|6|1000|1e-1|90.83|\n",
    "|7|100|1e-2|78.35|\n",
    "|8|500|1e-2|85.02|\n",
    "|9|1000|1e-2|87.07|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the highest accuracy when learning rate was set to 1. The accuraccy decreases as the learning rate decreases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
