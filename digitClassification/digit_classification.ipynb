{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic Regression for Digit Classification on mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n"
     ]
    }
   ],
   "source": [
    "# loading the training datasets\n",
    "\n",
    "x_traindata = loadmat(\"mnistTrainImages.mat\")\n",
    "y_trainlabels = loadmat(\"mnistTrainLabels.mat\")\n",
    "\n",
    "# Extracting the relevant information from the datasets\n",
    "\n",
    "x_traindata = x_traindata[\"trainData\"]\n",
    "y_trainlabels = y_trainlabels[\"trainLabels\"]\n",
    "\n",
    "print x_traindata.shape\n",
    "print y_trainlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the test datasets\n",
    "\n",
    "x_testdata = loadmat(\"mnistTestImages.mat\")\n",
    "y_testlabels = loadmat(\"mnistTestLabels.mat\")\n",
    "\n",
    "# Extracting the relevant information from the datase\n",
    "\n",
    "x_testdata = x_testdata[\"testData\"]\n",
    "y_testlabels = y_testlabels[\"testLabels\"]\n",
    "\n",
    "#print x_testdata.shape\n",
    "#print type(y_testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# converting the list of lists to a numpy array\n",
    "\n",
    "y_trainlabels = [i[0] for i in y_trainlabels]\n",
    "y_trainlabels = np.array(y_trainlabels)\n",
    "print y_trainlabels.shape\n",
    "\n",
    "y_testlabels = [i[0] for i in y_testlabels]\n",
    "y_testlabels = np.array(y_testlabels)\n",
    "print y_testlabels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_label = []\n",
    "for i in range(10):\n",
    "    temp = np.copy(y_trainlabels)\n",
    "    for j in range(len(temp)):\n",
    "        if temp[j] == i:\n",
    "            temp[j] = 1\n",
    "        else:\n",
    "            temp[j] = 0\n",
    "    store_label.append(temp)\n",
    "\n",
    "#print store_label\n",
    "    \n",
    "#print store_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the parameters\n",
    "\n",
    "theta = []\n",
    "opt_theta = []\n",
    "oldtheta  = []\n",
    "iteration = []\n",
    "\n",
    "for i in range(10):\n",
    "    theta.append(np.ones(x_traindata[0].shape))\n",
    "    oldtheta.append(np.inf)\n",
    "    \n",
    "alpha = 0.005\n",
    "epsilon = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(x,theta_calc):\n",
    "    return 1/(1+(np.exp(-(np.matmul(theta_calc,np.transpose(x))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_loss_fun(x,theta_calc,y):\n",
    "    return np.matmul(np.transpose(x),(logit(x,theta_calc)-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Gradient_descent(learn_rate,tolerance,theta_calc,old_theta_calc,x,y):\n",
    "    iteration = 0\n",
    "    while(np.linalg.norm(theta_calc - old_theta_calc, ord=2) > tolerance):\n",
    "        iteration = iteration+1\n",
    "        old_theta_calc = theta_calc\n",
    "        theta_calc = old_theta_calc - learn_rate*calc_loss_fun(x,theta_calc,y)\n",
    "    return (theta_calc,iteration)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmacs/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration =  94\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    (theta1,iteration1) = Gradient_descent(alpha,epsilon,theta[i],oldtheta[i],x_traindata,store_label[i])\n",
    "    opt_theta.append(theta1)\n",
    "    iteration.append(iteration1)\n",
    "    \n",
    "print \"Iteration = \", np.sum(iteration)\n",
    "#print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i in range(10):\n",
    "    prediction.append(logit(x_testdata,opt_theta[i]))\n",
    "predicted_labels = np.argmax(prediction, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy\n",
    "\n",
    "correct_prediction = 0\n",
    "for i in range(y_testlabels.size):\n",
    "    if predicted_labels[i] == y_testlabels[i]:\n",
    "        correct_prediction = correct_prediction +1\n",
    "print \"Accuracy is \", correct_prediction/float(y_testlabels.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Observation - \n",
    "\n",
    "\n",
    "| Alpha(Learning_Rate) | Epsilon(Error_tollerance) | No._of_iteration | Accuracy |\n",
    "| :--------: | :-------: | :---------: | :-------:|\n",
    "| | | | |\n",
    "| 0.001 | 30 | 86 | 78.99 |\n",
    "| 0.001 | 20 | 92 | 79.37 |\n",
    "| 0.001 | 10 | 360 | 81.83 |\n",
    "| 0.001 | 05 | 621 | 83.34 |\n",
    "| | | | |\n",
    "| 0.01 | 50 | 290 | 80.48 |\n",
    "| 0.01 | 40 | 508 | 82.05 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that using logistic regression for classifying k classes performs poorly compared to binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the error tollerance decreases the number of iteration it takes to converge increases drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model for learning rate 0.01 for error tollerance lesser than 40 takes very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
