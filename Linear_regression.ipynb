{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will try to define a linear relationship between the **input variable (X)**  and **output variable(Y)**. The input variable have multiple attributes, say ***n***. Then we can define a linear model as,\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n $$\n",
    "\n",
    "where $\\beta_0$ is the bias term. We can write the above expression as,\n",
    "\n",
    "$$ Y = \\beta_0 x_0 + \\beta_1 x_1 + ... + \\beta_n x_n $$\n",
    "\n",
    "where $x_0 = 1$. Thus we can write an equivalent of the equation in a matrix form.\n",
    "\n",
    "$$ Y = \\beta^T X $$\n",
    "\n",
    "where $\\beta = [\\beta_0 , \\beta_1, ... , \\beta_n]^T $ and $ X = [x_0, x_1, ... , x_n]^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the above linear model, we would need a hypothesis function. Let us define our hypothesis function as, \n",
    "\n",
    "$$ h_\\beta(x) = \\beta^Tx $$\n",
    "\n",
    "We now need a cost function to estimate the error in our model. So let our cost function be defined as,\n",
    "\n",
    "$$ J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m}(h_\\beta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "By minimizing the cost we can find the coefficients $\\beta$. We will be using ***Gradient Descent*** method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "\n",
    "print data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAFgCAYAAABEyiulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd4e47da90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting the scores as scatter plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(data.target)\n",
    "plt.xlabel('price ($1000s)')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (379, 13)\n",
      "y_train shape = (379,)\n",
      "X_test shape = (127, 13)\n",
      "y_test shape = (127,)\n"
     ]
    }
   ],
   "source": [
    "# splitting the dataset into train and test datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "\n",
    "print \"X_train shape =\", X_train.shape\n",
    "print \"y_train shape =\", y_train.shape\n",
    "\n",
    "print \"X_test shape =\", X_test.shape\n",
    "print \"y_test shape =\", y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X_train)\n",
    "x0 = np.ones((m,1))\n",
    "X = np.hstack((x0,X_train))\n",
    "Y = np.array(y_train)\n",
    "B = np.array(np.zeros(14))\n",
    "alpha = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "\n",
    "def cost_function(X,Y,B):\n",
    "    m = len(Y)\n",
    "    J = np.sum((np.matmul(X,B) - Y)**2) / (2*m)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298.361015831\n"
     ]
    }
   ],
   "source": [
    "cost_initial = cost_function(X,Y,B)\n",
    "print cost_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our initial cost of the model is high so we will use gradient descent method to reduce the cost.\n",
    "\n",
    "*** Hypothesis *** = $ h_\\beta(x) = \\beta^Tx $\n",
    "\n",
    "*** Loss *** = $ h_\\beta(x) - y $\n",
    "\n",
    "*** Gradient *** = $ (h_\\beta(x) - y)x_i $\n",
    "\n",
    "*** Gradient Descent Updation *** = $ \\beta_i:= \\beta_i - \\alpha((h_\\beta(x) - y)x_i) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Grad_descent(X,Y,B,alpha,iterations):\n",
    "    cost = [0]*iterations\n",
    "    m = len(Y)\n",
    "    for i in range(iterations):\n",
    "        hypo = np.matmul(X,B)\n",
    "        loss = hypo - Y\n",
    "        grad = np.matmul(loss,X)/m\n",
    "        B = B - (alpha * grad)\n",
    "        J = cost_function(X, Y, B)\n",
    "        cost[i] = J\n",
    "    return B, cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Beta =  [ 0.00555879 -0.07363869  0.13714733 -0.03204413  0.00420894  0.00181696\n",
      "  0.06640869  0.06212129  0.01074779  0.00143493  0.00262743  0.0502426\n",
      "  0.04750548 -0.18932836]\n",
      "final cost =  30.4429502197\n"
     ]
    }
   ],
   "source": [
    "finalB, finalCost = Grad_descent(X,Y,B,alpha,100000)\n",
    "\n",
    "print \"Final Beta = \", finalB\n",
    "\n",
    "print \"final cost = \", finalCost[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error(RMSE) =  8.41947930199\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation - MSE\n",
    "def rmse(Y_actual, Y_predicted):\n",
    "    error = np.sqrt(sum((Y_actual - Y_predicted) ** 2) / len(Y_actual))\n",
    "    return error\n",
    "\n",
    "n = len(X_test)\n",
    "x0 = np.ones((n,1))\n",
    "X_test = (np.hstack((x0,X_test)))\n",
    "\n",
    "Y_predicted = np.matmul(X_test,finalB)\n",
    "\n",
    "print \" Error(RMSE) = \", rmse(y_test,Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| s.no | No. of Iterations | learning_rate (alpha) | Error (RMSE)|\n",
    "|:---:|:-------:|:------:|:-----:|:----:|\n",
    "|1| 1000 | 1e-5 | nan|\n",
    "|2|1000| 1e-6 | 8.753|\n",
    "|3|1000| 1e-7 | 7.938 |\n",
    "|4| 1000| 1e-8 | 9.614|\n",
    "|5|10000|1e-5 | nan|\n",
    "|6| 10000| 1e-6 | 7.703|\n",
    "|7| 10000| 1e-7 | 8.607 |\n",
    "|8| 100000| 1e-5 | nan |\n",
    "|9| 100000| 1e-6 | 6.680 |\n",
    "|10| 100000| 1e-7 | 8.419 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We observe that the learning rate must be very low i.e atleast 1e-6 to achieve any result. We also observe that the error term oscillates this is due to the gradient descent function which is being used to reduce the loss incurred by the model.\n",
    "\n",
    "The error for 100000 iterations and learning rate 1e-6 is 6.680 which is close to the actual error 5.045 when linear regression is performed on the boston housing dataset using scikit learn package.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
